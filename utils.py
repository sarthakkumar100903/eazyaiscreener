# Enhanced utils.py — Multi-format Resume Parsing (PDF, DOCX, DOC), Embeddings, Contact Extraction, Azure Uploads
# Fixed for Streamlit Cloud compatibility

import re
import numpy as np
import tiktoken
import functools
import asyncio
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from azure.storage.blob import BlobClient, BlobServiceClient
from sklearn.metrics.pairwise import cosine_similarity
from constants import AZURE_CONFIG, MODEL_CONFIG, PERFORMANCE_CONFIG
from openai import AzureOpenAI
import pandas as pd
import io
import zipfile

# Import with fallback handling for cloud deployment
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False
    logging.warning("PyMuPDF not available - PDF parsing will be limited")

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    logging.warning("python-docx not available - DOCX parsing will be limited")

# Configure logging
logger = logging.getLogger(__name__)

# Initialize OpenAI client with optimized settings
client = AzureOpenAI(
    api_key=AZURE_CONFIG["openai_key"],
    api_version=AZURE_CONFIG["api_version"],
    azure_endpoint=AZURE_CONFIG["azure_endpoint"],
    max_retries=PERFORMANCE_CONFIG["max_retries"],
    timeout=PERFORMANCE_CONFIG["request_timeout"]
)

# ==========================
# 📄 Enhanced Multi-format Resume Text Extractor with Fallbacks
# ==========================

def parse_resume(file_bytes: bytes, filename: str = "resume", max_pages: int = 10) -> str:
    """
    Enhanced resume parser supporting PDF, DOCX, and DOC formats with cloud deployment compatibility
    """
    try:
        start_time = time.time()
        
        # Determine file type from filename or content
        file_extension = filename.lower().split('.')[-1] if '.' in filename else 'pdf'
        
        if file_extension == 'pdf':
            text = parse_pdf_with_fallback(file_bytes, max_pages)
        elif file_extension in ['docx', 'doc']:
            text = parse_word_document_with_fallback(file_bytes, file_extension)
        else:
            # Fallback: try to detect format by content
            text = parse_with_format_detection(file_bytes, max_pages)
        
        # Clean up text
        text = clean_resume_text(text)
        
        processing_time = time.time() - start_time
        logger.debug(f"Parsed {file_extension.upper()} in {processing_time:.2f}s, {len(text)} chars")
        
        return text
        
    except Exception as e:
        logger.error(f"Resume parsing failed for {filename}: {str(e)}")
        return f"Error reading resume: {str(e)}"

def parse_pdf_with_fallback(file_bytes: bytes, max_pages: int = 10) -> str:
    """Parse PDF files with fallback for cloud deployment"""
    if PYMUPDF_AVAILABLE:
        try:
            return parse_pdf_pymupdf(file_bytes, max_pages)
        except Exception as e:
            logger.warning(f"PyMuPDF parsing failed, trying fallback: {str(e)}")
    
    # Fallback method for PDF parsing
    return parse_pdf_basic_fallback(file_bytes)

def parse_pdf_pymupdf(file_bytes: bytes, max_pages: int = 10) -> str:
    """Parse PDF files using PyMuPDF (when available)"""
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            text_parts = []
            pages_processed = 0
            
            for page_num, page in enumerate(doc):
                if pages_processed >= max_pages:
                    break
                    
                try:
                    page_text = page.get_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                        pages_processed += 1
                except Exception as e:
                    logger.warning(f"Error processing PDF page {page_num}: {str(e)}")
                    continue
            
            return "\n\n".join(text_parts)
    except Exception as e:
        logger.error(f"PyMuPDF parsing error: {str(e)}")
        raise

def parse_pdf_basic_fallback(file_bytes: bytes) -> str:
    """Basic PDF text extraction fallback"""
    try:
        # Very basic PDF text extraction attempt
        text_content = file_bytes.decode('latin-1', errors='ignore')
        
        # Look for readable text patterns in PDF structure
        text_matches = re.findall(r'\((.*?)\)', text_content)
        readable_text = []
        
        for match in text_matches:
            # Filter out binary data and keep readable text
            if len(match) > 5 and re.search(r'[a-zA-Z\s]{5,}', match):
                readable_text.append(match)
        
        if readable_text:
            extracted = ' '.join(readable_text)
            # Clean up common PDF artifacts
            extracted = re.sub(r'\\[rn]', ' ', extracted)
            extracted = re.sub(r'\\[a-z]+', ' ', extracted)
            return extracted
        else:
            return "PDF content could not be extracted. Please use DOCX format or ensure PDF is text-based."
            
    except Exception as e:
        logger.error(f"PDF fallback extraction failed: {str(e)}")
        return "PDF parsing failed. Please convert to DOCX format for better results."

def parse_word_document_with_fallback(file_bytes: bytes, file_extension: str) -> str:
    """Parse DOCX and DOC files with fallback methods"""
    if file_extension == 'docx':
        if DOCX_AVAILABLE:
            try:
                return parse_docx_with_library(file_bytes)
            except Exception as e:
                logger.warning(f"python-docx parsing failed, trying fallback: {str(e)}")
        
        # Fallback to ZIP extraction
        return parse_docx_as_zip_fallback(file_bytes)
    
    elif file_extension == 'doc':
        return parse_doc_fallback(file_bytes)
    
    else:
        raise ValueError(f"Unsupported Word format: {file_extension}")

def parse_docx_with_library(file_bytes: bytes) -> str:
    """Parse DOCX files using python-docx library (when available)"""
    try:
        # Create a BytesIO object from bytes
        doc_io = io.BytesIO(file_bytes)
        doc = Document(doc_io)
        
        text_parts = []
        
        # Extract text from paragraphs
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_parts.append(paragraph.text.strip())
        
        # Extract text from tables
        for table in doc.tables:
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        row_text.append(cell_text)
                if row_text:
                    text_parts.append(" | ".join(row_text))
        
        return "\n".join(text_parts)
        
    except Exception as e:
        logger.error(f"python-docx parsing error: {str(e)}")
        raise

def parse_docx_as_zip_fallback(file_bytes: bytes) -> str:
    """Enhanced fallback method to extract DOCX content as ZIP"""
    try:
        doc_io = io.BytesIO(file_bytes)
        
        with zipfile.ZipFile(doc_io, 'r') as zip_file:
            # Try to read the main document content
            try:
                content = zip_file.read('word/document.xml')
                # Enhanced XML content extraction
                text = extract_text_from_xml(content.decode('utf-8', errors='ignore'))
                if len(text.strip()) > 50:  # Ensure we got meaningful content
                    return text
            except KeyError:
                pass
            
            # If document.xml is not found or empty, try other files
            text_parts = []
            xml_files = [f for f in zip_file.namelist() if f.startswith('word/') and f.endswith('.xml')]
            
            for filename in xml_files:
                try:
                    content = zip_file.read(filename)
                    clean_text = extract_text_from_xml(content.decode('utf-8', errors='ignore'))
                    if clean_text and len(clean_text.strip()) > 20:
                        text_parts.append(clean_text)
                except Exception:
                    continue
            
            if text_parts:
                return "\n".join(text_parts)
            else:
                return "DOCX content extraction failed. Document may be empty or corrupted."
                
    except Exception as e:
        logger.error(f"DOCX ZIP extraction failed: {str(e)}")
        return f"DOCX parsing error: {str(e)}. Please try converting to PDF format."

def extract_text_from_xml(xml_content: str) -> str:
    """Extract readable text from XML content"""
    try:
        # Remove XML tags but preserve text content
        # Handle common Word XML patterns
        text = re.sub(r'<w:t[^>]*>(.*?)</w:t>', r'\1', xml_content)
        text = re.sub(r'<w:p[^>]*>', '\n', text)  # Paragraphs become newlines
        text = re.sub(r'<[^>]+>', ' ', text)  # Remove remaining tags
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n', text)
        
        return text.strip()
    except Exception as e:
        logger.warning(f"XML text extraction error: {str(e)}")
        return ""

def parse_doc_fallback(file_bytes: bytes) -> str:
    """
    Enhanced fallback parser for DOC files with better error handling
    """
    try:
        # Try to extract text using various methods
        
        # Method 1: Look for readable text patterns
        text_content = file_bytes.decode('latin-1', errors='ignore')
        
        # Find text that looks like readable content
        text_patterns = []
        
        # Look for sequences of readable characters
        readable_sequences = re.findall(r'[a-zA-Z\s.,!?;:()-]{20,}', text_content)
        for seq in readable_sequences:
            # Filter out sequences that are mostly symbols or numbers
            if re.search(r'[a-zA-Z]{10,}', seq):
                text_patterns.append(seq)
        
        if text_patterns:
            extracted_text = '\n'.join(text_patterns)
            # Clean up the extracted text
            extracted_text = re.sub(r'[^\w\s.,!?;:()-]', ' ', extracted_text)
            extracted_text = re.sub(r'\s+', ' ', extracted_text)
            
            if len(extracted_text.strip()) > 100:
                return extracted_text.strip()
        
        # Method 2: Try UTF-8 with errors ignored
        try:
            text_utf8 = file_bytes.decode('utf-8', errors='ignore')
            readable_parts = re.findall(r'[a-zA-Z\s.,!?;:()-]{30,}', text_utf8)
            if readable_parts and len(' '.join(readable_parts)) > 100:
                return ' '.join(readable_parts)
        except:
            pass
        
        # If all methods fail, return informative message
        return """DOC format detected but full content extraction is limited on this platform. 

For best results, please:
1. Convert your resume to PDF or DOCX format
2. Or copy and paste the content into a new DOCX file

The system can partially read this document but may miss some content."""
        
    except Exception as e:
        logger.error(f"DOC parsing error: {str(e)}")
        return f"DOC file parsing failed: {str(e)}. Please convert to PDF or DOCX format."

def parse_with_format_detection(file_bytes: bytes, max_pages: int = 10) -> str:
    """Detect format and parse accordingly with enhanced fallbacks"""
    try:
        # Check for PDF signature
        if file_bytes.startswith(b'%PDF'):
            return parse_pdf_with_fallback(file_bytes, max_pages)
        
        # Check for DOCX signature (ZIP file with specific content)
        elif file_bytes.startswith(b'PK\x03\x04'):
            try:
                # Try to parse as DOCX first
                return parse_docx_as_zip_fallback(file_bytes)
            except:
                # If DOCX parsing fails, might be another ZIP format
                return "ZIP-based document format detected but content extraction failed. Please use PDF format."
        
        # Check for DOC signature
        elif file_bytes.startswith(b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1'):
            return parse_doc_fallback(file_bytes)
        
        else:
            # Try as text file
            try:
                text_content = file_bytes.decode('utf-8', errors='ignore')
                if len(text_content.strip()) > 50:
                    return text_content
                else:
                    return "Unknown file format - could not extract readable content"
            except:
                return "Unknown file format - binary content detected"
                
    except Exception as e:
        logger.error(f"Format detection failed: {str(e)}")
        return f"Error detecting file format: {str(e)}"

def clean_resume_text(text: str) -> str:
    """Clean and normalize resume text"""
    if not text:
        return ""
    
    # Remove excessive whitespace
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)  # Max 2 consecutive newlines
    text = re.sub(r'[ \t]+', ' ', text)  # Normalize spaces and tabs
    
    # Remove common document artifacts
    text = re.sub(r'[^\x20-\x7E\n]', ' ', text)  # Remove non-ASCII except newlines
    text = re.sub(r'(\w)\s+(\w)', r'\1 \2', text)  # Fix broken words
    
    # Clean up common Word/PDF artifacts
    text = re.sub(r'\x0c', ' ', text)  # Form feed characters
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', ' ', text)  # Control characters
    
    # Normalize line endings
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    return text.strip()

# ==========================
# 📎 Enhanced Text Chunking
# ==========================

@functools.lru_cache(maxsize=1)
def get_tokenizer():
    """Cached tokenizer for performance"""
    return tiktoken.encoding_for_model("gpt-4")

def chunk_text(text: str, max_tokens: int = None, overlap: int = None) -> List[str]:
    """
    Enhanced text chunking with configurable parameters
    """
    if max_tokens is None:
        max_tokens = PERFORMANCE_CONFIG["chunk_size"]
    if overlap is None:
        overlap = PERFORMANCE_CONFIG["chunk_overlap"]
    
    if not text:
        return [""]
    
    try:
        enc = get_tokenizer()
        tokens = enc.encode(text)
        
        if len(tokens) <= max_tokens:
            return [text]
        
        chunks = []
        i = 0
        while i < len(tokens):
            chunk_tokens = tokens[i:i + max_tokens]
            chunk_text = enc.decode(chunk_tokens)
            chunks.append(chunk_text)
            i += max_tokens - overlap
            
            # Limit number of chunks for performance
            if len(chunks) >= PERFORMANCE_CONFIG["max_resume_chunks"]:
                break
        
        return chunks
        
    except Exception as e:
        logger.error(f"Text chunking failed: {str(e)}")
        return [text[:3000]]  # Fallback to simple truncation

def get_text_chunks(text: str, max_tokens: int = 800, overlap: int = 100) -> List[str]:
    """Wrapper for backward compatibility"""
    return chunk_text(text, max_tokens, overlap)

# ==========================
# 🧠 Enhanced Embedding & Similarity
# ==========================

@functools.lru_cache(maxsize=100)  # Increased cache size
def get_embedding_cached(text: str) -> Tuple[float, ...]:
    """
    Cached embedding generation with error handling and performance optimization
    """
    try:
        # Truncate text for faster embedding generation
        if len(text) > 8000:  # Embedding model limit is ~8191 tokens
            text = text[:8000]
        
        start_time = time.time()
        
        response = client.embeddings.create(
            input=[text],
            model=MODEL_CONFIG["embedding_model"]
        )
        
        embedding = response.data[0].embedding
        processing_time = time.time() - start_time
        
        logger.debug(f"Generated embedding in {processing_time:.2f}s")
        
        return tuple(embedding)  # Convert to tuple for caching
        
    except Exception as e:
        logger.error(f"Embedding generation failed: {str(e)}")
        # Return a zero vector as fallback
        return tuple([0.0] * 1536)

def get_embedding(text: str) -> List[float]:
    """Non-cached embedding for compatibility"""
    return list(get_embedding_cached(text))

def get_cosine_similarity(vec1: Tuple[float, ...], vec2: Tuple[float, ...]) -> float:
    """
    Enhanced cosine similarity calculation with validation
    """
    try:
        if not vec1 or not vec2:
            return 0.0
            
        if len(vec1) != len(vec2):
            logger.warning(f"Vector dimension mismatch: {len(vec1)} vs {len(vec2)}")
            return 0.0
        
        # Convert tuples to numpy arrays for calculation
        v1 = np.array(vec1).reshape(1, -1)
        v2 = np.array(vec2).reshape(1, -1)
        
        # Handle zero vectors
        if np.allclose(v1, 0) or np.allclose(v2, 0):
            return 0.0
        
        similarity = cosine_similarity(v1, v2)[0][0]
        
        # Ensure result is in valid range
        return max(0.0, min(1.0, float(similarity)))
        
    except Exception as e:
        logger.error(f"Cosine similarity calculation failed: {str(e)}")
        return 0.0

# ==========================
# 📬 Enhanced Contact Info Extractor
# ==========================

def extract_contact_info(text: str) -> Dict[str, str]:
    """
    Enhanced contact information extraction with multiple fallback strategies
    """
    if not text:
        return {"name": "N/A", "email": "N/A", "phone": "N/A"}
    
    contact_info = {
        "name": "N/A",
        "email": "N/A", 
        "phone": "N/A"
    }
    
    # Extract email with improved pattern
    email_patterns = [
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Standard email
        r'[A-Za-z0-9._%+-]+\s*@\s*[A-Za-z0-9.-]+\s*\.\s*[A-Za-z]{2,}',  # Email with spaces
    ]
    
    for pattern in email_patterns:
        email_match = re.search(pattern, text)
        if email_match:
            contact_info["email"] = email_match.group(0).replace(" ", "")
            break
    
    # Extract phone with improved patterns
    phone_patterns = [
        r'\+?\d{1,3}[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',  # Various formats
        r'\+?\d{1,3}[-.\s]?\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',        # International
        r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',                   # US format
        r'\+?\d{10,15}',                                          # Simple digits
    ]
    
    for pattern in phone_patterns:
        phone_match = re.search(pattern, text)
        if phone_match:
            phone = phone_match.group(0)
            # Clean phone number
            phone = re.sub(r'[^\d+]', '', phone)
            if len(phone) >= 10:
                contact_info["phone"] = phone
                break
    
    # Enhanced name extraction
    contact_info["name"] = extract_candidate_name(text)
    
    return contact_info

def extract_candidate_name(text: str) -> str:
    """
    Enhanced name extraction with multiple strategies
    """
    if not text:
        return "N/A"
    
    lines = text.strip().split('\n')
    
    # Strategy 1: Look for name in first few lines
    for i, line in enumerate(lines[:5]):
        line = line.strip()
        if not line:
            continue
            
        # Skip lines with common resume headers
        skip_patterns = [
            r'resume|cv|curriculum|vitae|profile|objective|summary',
            r'contact|information|details|phone|email|address',
            r'experience|education|skills|projects|work|employment'
        ]
        
        if any(re.search(pattern, line, re.I) for pattern in skip_patterns):
            continue
        
        # Look for capitalized words that could be names
        words = line.split()
        if 2 <= len(words) <= 4:  # Typical name length
            if all(word[0].isupper() for word in words if word.isalpha()):
                # Additional validation
                if not any(char.isdigit() for char in line):  # No numbers in name
                    return line.strip()
    
    # Strategy 2: Look for "Name:" pattern
    name_match = re.search(r'name\s*:?\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', text, re.I)
    if name_match:
        return name_match.group(1).strip()
    
    # Strategy 3: Find first line with proper capitalization
    for line in lines[:10]:
        line = line.strip()
        if len(line.split()) == 2:  # Likely first and last name
            words = line.split()
            if all(word[0].isupper() and word[1:].islower() for word in words):
                return line
    
    return "N/A"

# ==========================
# ☁️ Enhanced Azure Uploads with Error Handling
# ==========================

def upload_to_blob(file_bytes: bytes, file_name: str, container: str, 
                  overwrite: bool = True, max_retries: int = 3) -> bool:
    """
    Enhanced blob upload with retry logic and error handling
    """
    if not file_bytes or not file_name:
        logger.error("Invalid file data for blob upload")
        return False
    
    for attempt in range(max_retries):
        try:
            blob = BlobClient.from_connection_string(
                conn_str=AZURE_CONFIG["connection_string"],
                container_name=container,
                blob_name=file_name
            )
            
            blob.upload_blob(file_bytes, overwrite=overwrite)
            logger.debug(f"Successfully uploaded {file_name} to {container}")
            return True
            
        except Exception as e:
            logger.warning(f"Blob upload attempt {attempt + 1} failed: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(1.0 * (attempt + 1))  # Exponential backoff
            else:
                logger.error(f"Failed to upload {file_name} after {max_retries} attempts")
                return False
    
    return False

def save_summary_to_blob(pdf_bytes: bytes, file_name: str, container: str) -> bool:
    """Save PDF summary to blob storage"""
    return upload_to_blob(pdf_bytes, file_name, container)

def save_csv_to_blob(df: pd.DataFrame, file_name: str, container: str) -> bool:
    """Save CSV data to blob storage"""
    try:
        csv_data = df.to_csv(index=False).encode('utf-8')
        return upload_to_blob(csv_data, file_name, container)
    except Exception as e:
        logger.error(f"Failed to convert DataFrame to CSV: {str(e)}")
        return False

def download_from_blob(file_name: str, container: str) -> Optional[bytes]:
    """Download file from blob storage"""
    try:
        blob = BlobClient.from_connection_string(
            conn_str=AZURE_CONFIG["connection_string"],
            container_name=container,
            blob_name=file_name
        )
        
        return blob.download_blob().readall()
        
    except Exception as e:
        logger.error(f"Failed to download {file_name} from {container}: {str(e)}")
        return None

# ==========================
# 🔧 Utility Functions
# ==========================

def validate_resume_content(text: str) -> Dict[str, Any]:
    """
    Validate resume content and return quality metrics
    """
    if not text:
        return {
            "is_valid": False,
            "issues": ["Empty resume content"],
            "word_count": 0,
            "has_contact": False
        }
    
    word_count = len(text.split())
    issues = []
    
    # Check minimum length
    if len(text) < 100:
        issues.append("Resume too short")
    
    # Check maximum length  
    if len(text) > 50000:
        issues.append("Resume too long")
    
    # Check for contact information
    contact = extract_contact_info(text)
    has_contact = contact["email"] != "N/A" or contact["phone"] != "N/A"
    
    if not has_contact:
        issues.append("Missing contact information")
    
    # Check for suspicious patterns
    suspicious_patterns = [
        r'lorem ipsum', r'sample text', r'placeholder', 
        r'your name here', r'example\.com'
    ]
    
    for pattern in suspicious_patterns:
        if re.search(pattern, text, re.I):
            issues.append("Contains placeholder text")
            break
    
    return {
        "is_valid": len(issues) == 0,
        "issues": issues,
        "word_count": word_count,
        "character_count": len(text),
        "has_contact": has_contact,
        "contact_info": contact
    }

def get_file_format_from_name(filename: str) -> str:
    """Extract file format from filename"""
    if '.' in filename:
        return filename.lower().split('.')[-1]
    return 'unknown'

def is_supported_resume_format(filename: str) -> bool:
    """Check if file format is supported for resume parsing"""
    supported_formats = ['pdf', 'docx', 'doc']
    file_format = get_file_format_from_name(filename)
    return file_format in supported_formats

def format_processing_time(seconds: float) -> str:
    """Format processing time in human-readable format"""
    if seconds < 1:
        return f"{seconds*1000:.0f}ms"
    elif seconds < 60:
        return f"{seconds:.1f}s"
    else:
        minutes = int(seconds // 60)
        remaining_seconds = seconds % 60
        return f"{minutes}m {remaining_seconds:.1f}s"

def calculate_throughput(resume_count: int, total_time: float) -> float:
    """Calculate processing throughput in resumes per hour"""
    if total_time <= 0:
        return 0.0
    return (resume_count / total_time) * 3600

# ==========================
# 📊 Enhanced File Format Statistics
# ==========================

def analyze_file_formats(file_list: List[Tuple[str, bytes]]) -> Dict[str, Any]:
    """Analyze file formats in a list of files"""
    format_stats = {}
    total_size = 0
    
    for filename, file_bytes in file_list:
        file_format = get_file_format_from_name(filename)
        file_size = len(file_bytes)
        
        if file_format not in format_stats:
            format_stats[file_format] = {
                'count': 0,
                'total_size': 0,
                'files': []
            }
        
        format_stats[file_format]['count'] += 1
        format_stats[file_format]['total_size'] += file_size
        format_stats[file_format]['files'].append(filename)
        total_size += file_size
    
    # Calculate percentages
    for format_info in format_stats.values():
        format_info['size_percentage'] = (format_info['total_size'] / total_size) * 100 if total_size > 0 else 0
    
    return {
        'format_breakdown': format_stats,
        'total_files': len(file_list),
        'total_size_bytes': total_size,
        'total_size_mb': total_size / (1024 * 1024),
        'supported_formats': [fmt for fmt in format_stats.keys() if fmt in ['pdf', 'docx', 'doc']],
        'unsupported_formats': [fmt for fmt in format_stats.keys() if fmt not in ['pdf', 'docx', 'doc']]
    }

# ==========================
# 🔍 Advanced Text Processing for Multi-format Support
# ==========================

def extract_skills_from_text(text: str) -> List[str]:
    """Extract potential skills from resume text"""
    # Common technical skills patterns
    skill_patterns = [
        r'\b(?:Python|Java|JavaScript|C\+\+|C#|Ruby|PHP|Go|Rust|Kotlin|Swift)\b',
        r'\b(?:React|Angular|Vue|Node\.js|Django|Flask|Spring|Laravel)\b',
        r'\b(?:AWS|Azure|GCP|Docker|Kubernetes|Jenkins|Git|Linux|Windows)\b',
        r'\b(?:SQL|MySQL|PostgreSQL|MongoDB|Redis|Elasticsearch)\b',
        r'\b(?:HTML|CSS|REST|GraphQL|API|JSON|XML)\b'
    ]
    
    skills = set()
    for pattern in skill_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        skills.update(matches)
    
    return sorted(list(skills))

def extract_experience_years(text: str) -> Optional[int]:
    """Extract years of experience from resume text"""
    patterns = [
        r'(\d+)[\+\s]*years?\s+(?:of\s+)?experience',
        r'(\d+)[\+\s]*yrs?\s+(?:of\s+)?experience',
        r'experience[:\s]+(\d+)[\+\s]*years?',
        r'(\d+)[\+\s]*years?\s+in\s+(?:the\s+)?field'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except ValueError:
                continue
    
    return None

def extract_education_level(text: str) -> str:
    """Extract highest education level from resume text"""
    education_patterns = [
        (r'\b(?:PhD|Ph\.D|Doctorate|Doctoral)\b', 'PhD'),
        (r'\b(?:Masters?|M\.S|M\.A|MBA|M\.Tech|M\.E)\b', 'Masters'),
        (r'\b(?:Bachelor|B\.S|B\.A|B\.Tech|B\.E)\b', 'Bachelors'),
        (r'\b(?:Associate|A\.S|A\.A)\b', 'Associates'),
        (r'\b(?:High School|Diploma|12th)\b', 'High School')
    ]
    
    for pattern, level in education_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return level
    
    return "Not Specified"

# ==========================
# 📈 Quality Scoring with Multi-format Considerations
# ==========================

def calculate_resume_quality_score(text: str, contact: Dict[str, str], file_format: str = "pdf") -> Dict[str, Any]:
    """Calculate overall resume quality score with format-specific adjustments"""
    if not text:
        return {"score": 0, "factors": ["Empty resume"], "format": file_format}
    
    factors = []
    score = 0
    
    # Format-specific adjustments
    format_bonus = {
        'pdf': 0,    # Standard format
        'docx': 0,   # Standard format  
        'doc': -5    # Older format, slight penalty
    }
    
    score += format_bonus.get(file_format, -10)  # Unknown formats get penalty
    
    # Length check (10 points)
    word_count = len(text.split())
    if 200 <= word_count <= 2000:
        score += 10
    elif word_count < 200:
        factors.append("Too short")
    else:
        factors.append("Too long")
    
    # Contact information (20 points)
    contact_score = 0
    if contact.get("name", "N/A") != "N/A":
        contact_score += 7
    if contact.get("email", "N/A") != "N/A":
        contact_score += 7
    if contact.get("phone", "N/A") != "N/A":
        contact_score += 6
    score += contact_score
    
    if contact_score < 20:
        factors.append("Missing contact information")
    
    # Structure check (20 points)
    structure_indicators = [
        r'(?:experience|work|employment)',
        r'(?:education|academic)',
        r'(?:skills|competenc)',
        r'(?:project|portfolio)'
    ]
    
    structure_score = 0
    for indicator in structure_indicators:
        if re.search(indicator, text, re.IGNORECASE):
            structure_score += 5
    
    score += min(structure_score, 20)
    if structure_score < 15:
        factors.append("Poor structure")
    
    # Content quality (30 points)
    content_indicators = [
        (r'\b\d{4}\b', 5, "Has dates"),  # Years/dates
        (r'[%\d]+%|\d+\%', 5, "Has metrics"),  # Percentages/metrics
        (r'(?:achieved|improved|increased|developed|created|led|managed)', 10, "Action words"),
        (r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Inc|Corp|LLC|Ltd|Company)', 10, "Company names")
    ]
    
    content_score = 0
    for pattern, points, description in content_indicators:
        if re.search(pattern, text, re.IGNORECASE):
            content_score += points
    
    score += min(content_score, 30)
    if content_score < 20:
        factors.append("Lacks specific details")
    
    # Technical content (20 points)
    skills = extract_skills_from_text(text)
    tech_score = min(len(skills) * 2, 20)
    score += tech_score
    
    if tech_score < 10:
        factors.append("Limited technical skills mentioned")
    
    return {
        "score": min(max(score, 0), 100),  # Ensure 0-100 range
        "word_count": word_count,
        "skills_found": skills,
        "factors": factors,
        "contact_completeness": contact_score / 20 * 100,
        "file_format": file_format,
        "format_quality": "Good" if file_format in ['pdf', 'docx'] else "Acceptable" if file_format == 'doc' else "Poor"
    }

# ==========================
# 🚨 Enhanced Fraud Detection with Multi-format Support
# ==========================

def detect_resume_anomalies(text: str, contact: Dict[str, str], file_format: str = "unknown") -> Dict[str, Any]:
    """Detect potential resume fraud or anomalies with format-specific checks"""
    anomalies = []
    risk_score = 0
    
    if not text:
        return {"anomalies": ["Empty resume"], "risk_score": 100, "format": file_format}
    
    # Format-specific risk assessment
    if file_format == 'doc':
        # DOC files have limited parsing, higher risk of incomplete analysis
        risk_score += 5
        anomalies.append("DOC format may have incomplete text extraction")
    elif file_format not in ['pdf', 'docx', 'doc']:
        risk_score += 15
        anomalies.append("Unusual file format for resumes")
    
    # Check for template indicators
    template_indicators = [
        r'lorem ipsum', r'sample text', r'placeholder',
        r'your name here', r'example\.com', r'\[.*\]',
        r'insert.*here', r'replace.*with'
    ]
    
    for pattern in template_indicators:
        if re.search(pattern, text, re.IGNORECASE):
            anomalies.append("Contains template/placeholder text")
            risk_score += 20
            break
    
    # Check for excessive claims
    excessive_claims = [
        r'expert in (?:all|every|100\+)',
        r'single-handedly (?:increased|improved|developed)',
        r'(?:increased|improved).*(?:1000|500)%',
        r'worked with (?:all|every) Fortune',
        r'(?:invented|pioneered|revolutionized)'
    ]
    
    for pattern in excessive_claims:
        if re.search(pattern, text, re.IGNORECASE):
            anomalies.append("Contains unrealistic claims")
            risk_score += 15
            break
    
    # Check for inconsistent information
    years_mentioned = re.findall(r'\b(19|20)\d{2}\b', text)
    if len(years_mentioned) > 10:  # Too many years mentioned
        anomalies.append("Excessive date references")
        risk_score += 10
    
    # Check contact information consistency
    emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
    if len(emails) > 3:
        anomalies.append("Multiple email addresses")
        risk_score += 10
    
    # Check for duplicate content
    sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 20]
    if len(sentences) != len(set(sentences)):
        anomalies.append("Contains duplicate content")
        risk_score += 15
    
    # Format-specific anomaly detection
    if file_format == 'pdf' and 'Error reading resume' in text:
        anomalies.append("PDF parsing errors detected")
        risk_score += 10
    elif file_format in ['docx', 'doc'] and len(text) < 100:
        anomalies.append("Word document appears to have minimal content")
        risk_score += 10
    
    return {
        "anomalies": anomalies,
        "risk_score": min(risk_score, 100),
        "requires_manual_review": risk_score > 30,
        "file_format": file_format,
        "format_specific_issues": [a for a in anomalies if 'format' in a.lower() or 'parsing' in a.lower()]
    }

# ==========================
# 📊 Export Utilities with Format Information
# ==========================

def prepare_export_data(df: pd.DataFrame, include_sensitive: bool = False) -> pd.DataFrame:
    """Prepare dataframe for export with optional sensitive data filtering"""
    export_df = df.copy()
    
    # Remove large text fields to reduce file size
    columns_to_remove = ["resume_text", "embedding"]
    if not include_sensitive:
        columns_to_remove.extend(["phone"])  # Remove phone for privacy
    
    export_df = export_df.drop(columns=columns_to_remove, errors='ignore')
    
    # Clean up list fields for CSV export
    list_columns = ["red_flags", "missing_gaps", "highlights", "reasons_if_rejected"]
    for col in list_columns:
        if col in export_df.columns:
            export_df[col] = export_df[col].apply(
                lambda x: "; ".join(x) if isinstance(x, list) else str(x)
            )
    
    # Add file format information if available
    if 'resume_file' in export_df.columns:
        export_df['file_format'] = export_df['resume_file'].apply(get_file_format_from_name)
    
    return export_df

def generate_analysis_summary(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate comprehensive analysis summary with format breakdown"""
    if not results:
        return {"error": "No results to analyze"}
    
    df = pd.DataFrame(results)
    
    # File format analysis
    format_breakdown = {}
    if 'resume_file' in df.columns:
        df['file_format'] = df['resume_file'].apply(get_file_format_from_name)
        format_breakdown = df['file_format'].value_counts().to_dict()
    
    summary = {
        "total_candidates": len(df),
        "average_score": df["score"].mean(),
        "score_distribution": {
            "excellent": len(df[df["score"] >= 80]),
            "good": len(df[(df["score"] >= 60) & (df["score"] < 80)]),
            "average": len(df[(df["score"] >= 40) & (df["score"] < 60)]),
            "poor": len(df[df["score"] < 40])
        },
        "verdict_distribution": df["verdict"].value_counts().to_dict(),
        "format_breakdown": format_breakdown,
        "top_skills": extract_top_skills_from_results(results),
        "quality_metrics": {
            "fraud_detected": len(df[df["fraud_detected"] == True]),
            "missing_contact": len(df[df["email"] == "N/A"]),
            "complete_profiles": len(df[(df["email"] != "N/A") & (df["phone"] != "N/A")])
        }
    }
    
    return summary

def extract_top_skills_from_results(results: List[Dict[str, Any]]) -> List[Dict[str, int]]:
    """Extract most mentioned skills across all resumes"""
    skill_counts = {}
    
    for result in results:
        resume_text = result.get("resume_text", "")
        if resume_text:
            skills = extract_skills_from_text(resume_text)
            for skill in skills:
                skill_counts[skill] = skill_counts.get(skill, 0) + 1
    
    # Sort by frequency and return top 10
    sorted_skills = sorted(skill_counts.items(), key=lambda x: x[1], reverse=True)
    return [{"skill": skill, "count": count} for skill, count in sorted_skills[:10]]

# ==========================
# 🔄 Library Status Functions for Debugging
# ==========================

def get_library_status() -> Dict[str, bool]:
    """Get status of optional libraries for debugging"""
    return {
        "pymupdf": PYMUPDF_AVAILABLE,
        "python_docx": DOCX_AVAILABLE,
    }

def get_supported_formats() -> List[str]:
    """Get list of fully supported formats based on available libraries"""
    formats = []
    
    if PYMUPDF_AVAILABLE:
        formats.append("pdf")
    else:
        formats.append("pdf (limited)")
    
    if DOCX_AVAILABLE:
        formats.extend(["docx", "doc"])
    else:
        formats.extend(["docx (limited)", "doc (limited)"])
    
    return formats

def log_parsing_capabilities():
    """Log current parsing capabilities for debugging"""
    logger.info(f"PDF parsing: {'Full (PyMuPDF)' if PYMUPDF_AVAILABLE else 'Limited (fallback)'}")
    logger.info(f"DOCX parsing: {'Full (python-docx)' if DOCX_AVAILABLE else 'Limited (ZIP extraction)'}")
    logger.info(f"DOC parsing: Limited (text extraction)")
    
    if not PYMUPDF_AVAILABLE:
        logger.warning("PyMuPDF not available - PDF parsing will be limited")
    if not DOCX_AVAILABLE:
        logger.warning("python-docx not available - DOCX parsing will use fallback methods")
